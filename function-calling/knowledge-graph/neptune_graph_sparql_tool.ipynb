{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3ea1471f",
   "metadata": {},
   "source": [
    "## Query Neptune Graph with SPARQL in Natural Language Using Function Calling\n",
    "\n",
    "This notebook will walk you through the steps to create a tool that can query Amazon Neptune database via SPARQL using function calling in Bedrock Converse API. The tool will be invoked when a user wants to get object litereal information from Amazon Neptune database for a specific object id. LLM(Large Language Model) will pass the required input parameter from the user's question the function *get_objectLiteral* and the fucntion will execute the SPARQL query to fetch the results. At the end LLM(Large Language Model) will give the final response to the user including the query results.\n",
    "We will use the Human Disease Ontology as a dataset to load to Amazon Neptune https://www.ebi.ac.uk/ols4/ontologies/doid\n",
    "\n",
    "\n",
    "\n",
    "### Please Compete the Prerequisites before you start!\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f6d67f8",
   "metadata": {},
   "source": [
    "## Prerequisites\n",
    "1.Please make sure that you complete all the prerequisites explained in this [link](https://docs.aws.amazon.com/neptune/latest/userguide/bulk-load-data.html)\n",
    "\n",
    "2.In the security group of Neptune Database, allow TCP connections from the port 8182(default port of the instance) through the subnet CIDR range where the database instance resides.\n",
    "\n",
    "3.This notebook will run as neptune notebook. During the configuration of database cluster allow the instance to create notebook.\n",
    "\n",
    "4.After the notebook is being created you can see it in the sagemaker console, under notebooks. The default IAM role of the notebook instance, by default, has list and read permissions for S3 in the policy. Edit the policy and add \"s3:PutObject\" in the actions and add your S3 bucket ARN in the Resources section of the IAM policy. In order to access Amazon Bedrock, attach *AmazonBedrockFullAccess* policy to the same role.\n",
    "\n",
    "After you complete all the steps you should have\n",
    "* Amazon Neptune database instance (Follow [creating an new neptune DB cluster](https://docs.aws.amazon.com/neptune/latest/userguide/get-started-create-cluster.html) or create manually from the AWS Console). You need to set up the cluster within VPC!\n",
    "* S3 bucket to save the sample ontology data\n",
    "* S3 VPC endpoint\n",
    "* IAM role for Neptune DB instance within the notebook which allows access to the data files in your S3 bucket and allows access to Amazon Bedrock\n",
    "*A security group attached to Neptune database which will allow TCP connections from the port 8182(default port of the instance) through the subnet CIDR range where the database instance resides.\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d3fa059",
   "metadata": {},
   "source": [
    "## Install and Import the required modules\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eca4adf8-b8c6-4c08-b1df-dd11708aff51",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip3 install -qU boto3\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "042cbf05",
   "metadata": {},
   "source": [
    "### It is important to use boto3 version equal or greater than 1.34.139!!!!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5097001c-5e68-417c-bdcf-d31baadc0e58",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "import boto3\n",
    "import json, sys\n",
    "from datetime import datetime\n",
    "\n",
    "print('Running boto3 version:', boto3.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39568edd-8b92-4b58-bfa2-e98ae0ba41a8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "modelId = 'anthropic.claude-3-sonnet-20240229-v1:0'\n",
    "#modelId = 'anthropic.claude-3-haiku-20240307-v1:0'\n",
    "#modelId = 'cohere.command-r-plus-v1:0'\n",
    "#modelId = 'cohere.command-r-v1:0'\n",
    "#modelId = 'mistral.mistral-large-2402-v1:0'\n",
    "print(f'Using modelId: {modelId}')\n",
    "\n",
    "region = 'us-east-1'\n",
    "print('Using region: ', region)\n",
    "\n",
    "bedrock = boto3.client(\n",
    "    service_name = 'bedrock-runtime',\n",
    "    region_name = region,\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6476ae5f",
   "metadata": {},
   "source": [
    "### After you create your S3 bucket add a folder named 'data'. You will download the Human disease ontology data into that folder in the cell below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "174dec80",
   "metadata": {},
   "outputs": [],
   "source": [
    "# URL of the OWL file to download\n",
    "owl_file_url = \"http://purl.obolibrary.org/obo/doid.owl\"\n",
    "\n",
    "# Download the OWL file\n",
    "response = requests.get(owl_file_url)\n",
    "\n",
    "# Check if the download was successful\n",
    "if response.status_code == 200:\n",
    "    # Create an S3 client\n",
    "    s3_client = boto3.client('s3')\n",
    "\n",
    "    # S3 bucket name\n",
    "    bucket_name = 's3bucketname'\n",
    "\n",
    "    # S3 object key (filename in the bucket)\n",
    "    object_key = 'data/doid.owl'\n",
    "\n",
    "    # Upload the file to S3\n",
    "    s3_client.put_object(Body=response.content, Bucket=bucket_name, Key=object_key)\n",
    "    \n",
    "    print(f\"File '{object_key}' uploaded to S3 bucket '{bucket_name}' successfully!\")\n",
    "else:\n",
    "    print(f\"Failed to download the file. Error: {response.status_code}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "320d9fc1",
   "metadata": {},
   "source": [
    "### Insert the writer endpoint of your database instance to check the connectivity, if it is not healthy please check the prerequisites again ! You can find the database endpoints from Amazon Neptune console\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80052c2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#checking database connectivity\n",
    "\n",
    "!curl https://{your_writer_endpoint}/status"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "179ba83c",
   "metadata": {},
   "source": [
    "### Replace the parameters in this command with your own parameters for neptune endpoint, region and IAM role\n",
    "\n",
    "curl -X POST \\\n",
    "    -H 'Content-Type: application/json' \\\n",
    "    https://your-neptune-endpoint:port/loader -d '\n",
    "    {\n",
    "      \"source\" : \"s3://bucket-name/object-key-name\",\n",
    "      \"format\" : \"format\",\n",
    "      \"iamRoleArn\" : \"arn:aws:iam::account-id:role/role-name\",\n",
    "      \"region\" : \"region\",\n",
    "      \"failOnError\" : \"FALSE\",\n",
    "      \"parallelism\" : \"MEDIUM\",\n",
    "      \"updateSingleCardinalityProperties\" : \"FALSE\",\n",
    "      \"queueRequest\" : \"TRUE\",\n",
    "      \"dependencies\" : [\"load_A_id\", \"load_B_id\"]\n",
    "    }'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cad105c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load the data from S3 to writer endpoint of your database instance\n",
    "!curl -X POST -H 'Content-Type: application/json' https://{writer-endpoint}:8182/loader -d '{\"source\" : \"{s3_url_of_data}\",\"format\" : \"rdfxml\",\"iamRoleArn\" : \"{arn_of_the_IAMrole}\",\"region\" : \"us-east-1\",\"failOnError\" : \"FALSE\",\"parallelism\" : \"MEDIUM\",\"updateSingleCardinalityProperties\" : \"FALSE\",\"queueRequest\" : \"TRUE\"}'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8712166b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#get neptune reader endoint\n",
    "neptune_endpoint='neptune_reader_endpoint'\n",
    "neptune_port='8182'\n",
    "url=f\"https://{neptune_endpoint}:{neptune_port}/sparql\"\n",
    "print('neptune_query_url: ',url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f85b0675",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the test SPARQL query\n",
    "query = \"\"\"\n",
    "    PREFIX : <http://purl.obolibrary.org/obo/>\n",
    "    SELECT ?objectLiteral\n",
    "    WHERE {\n",
    "        :DOID_9884 ?p ?objectLiteral .\n",
    "        FILTER(isLiteral(?objectLiteral))\n",
    "        FILTER(STRSTARTS(STR(?p), 'http://purl.obolibrary.org/obo/'))\n",
    "    }\n",
    "\"\"\"\n",
    "\n",
    "# Send the SPARQL query to the Neptune endpoint via POST request\n",
    "response = requests.post(url, data={\"query\":query})\n",
    "\n",
    "if response.status_code == 200:\n",
    "    print(response.json())\n",
    "else:\n",
    "    print(f\"Error: {response.status_code} - {response.text}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba328409",
   "metadata": {},
   "source": [
    "### Insert your neptune query url to the 'url' value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed35a519-1b8b-4f78-ad12-18f148d33452",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class ToolsList:\n",
    "    #Define our get_objectLiteral tool function...\n",
    "    def get_objectLiteral(self, id, predicate):\n",
    "        print(id, predicate)\n",
    "        url = '{insert_neptune_query_url}'\n",
    "        query = f\"\"\"\n",
    "            PREFIX : <{predicate}> \n",
    "            SELECT ?objectLiteral \n",
    "            WHERE {{:{id} ?p ?objectLiteral . \n",
    "            FILTER(isLiteral(?objectLiteral)) \n",
    "            FILTER(STRSTARTS(STR(?p), '{predicate}'))\n",
    "            }}\"\"\"\n",
    "        print(query)\n",
    "        response = requests.post(\n",
    "            f\"{url}\",\n",
    "            data={\"query\": query},\n",
    "        )\n",
    "        result = f'Value in {id}, {predicate} is ' + response.text\n",
    "        print(f'Tool result: {result}')\n",
    "        return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ffcb527-eb7e-49b3-9116-8a7bae3bb504",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#Define the configuration for our tool...\n",
    "toolConfig = {'tools': [],\n",
    "'toolChoice': {\n",
    "    'auto': {},\n",
    "    #'any': {},\n",
    "    #'tool': {\n",
    "    #    'name': 'get_weather'\n",
    "    #}\n",
    "    }\n",
    "}\n",
    "\n",
    "toolConfig['tools'].append({\n",
    "        'toolSpec': {\n",
    "            'name': 'get_objectLiteral',\n",
    "            'description': 'Get objectLiteral value of a given ID.',\n",
    "            'inputSchema': {\n",
    "                'json': {\n",
    "                    'type': 'object',\n",
    "                    'properties': {\n",
    "                        'id': {\n",
    "                            'type': 'string',\n",
    "                            'description': 'ID of the object'\n",
    "                        },\n",
    "                        'predicate': {\n",
    "                            'type': 'string',\n",
    "                            'description': 'predicate'\n",
    "                        }\n",
    "                    },\n",
    "                    'required': ['id', 'predicate']\n",
    "                }\n",
    "            }\n",
    "        }\n",
    "    })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a86b457-bbb2-46df-9f70-17d14b38d13c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#Function for caling the Bedrock Converse API...\n",
    "def converse_with_tools(messages, system='', toolConfig=toolConfig):\n",
    "    response = bedrock.converse(\n",
    "        modelId=modelId,\n",
    "        system=system,\n",
    "        messages=messages,\n",
    "        toolConfig=toolConfig\n",
    "    )\n",
    "    return response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32ff7a01-3088-4a98-93bd-e184406deb45",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#Function for orchestrating the conversation flow...\n",
    "def converse(prompt, system=''):\n",
    "    #Add the initial prompt:\n",
    "    messages = []\n",
    "    messages.append(\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": [\n",
    "                {\n",
    "                    \"text\": prompt\n",
    "                }\n",
    "            ]\n",
    "        }\n",
    "    )\n",
    "    print(f\"\\n{datetime.now().strftime('%H:%M:%S')} - Initial prompt:\\n{json.dumps(messages, indent=2)}\")\n",
    "\n",
    "    #Invoke the model the first time:\n",
    "    output = converse_with_tools(messages, system)\n",
    "    print(f\"\\n{datetime.now().strftime('%H:%M:%S')} - Output so far:\\n{json.dumps(output['output'], indent=2, ensure_ascii=False)}\")\n",
    "\n",
    "    #Add the intermediate output to the prompt:\n",
    "    messages.append(output['output']['message'])\n",
    "\n",
    "    function_calling = next((c['toolUse'] for c in output['output']['message']['content'] if 'toolUse' in c), None)\n",
    "\n",
    "    #Check if function calling is triggered:\n",
    "    if function_calling:\n",
    "        #Get the tool name and arguments:\n",
    "        tool_name = function_calling['name']\n",
    "        tool_args = function_calling['input'] or {}\n",
    "        \n",
    "        #Run the tool:\n",
    "        print(f\"\\n{datetime.now().strftime('%H:%M:%S')} - Running ({tool_name}) tool...\")\n",
    "        tool_response = getattr(ToolsList(), tool_name)(**tool_args) or \"\"\n",
    "        if tool_response:\n",
    "            tool_status = 'success'\n",
    "        else:\n",
    "            tool_status = 'error'\n",
    "\n",
    "        #Add the tool result to the prompt:\n",
    "        messages.append(\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": [\n",
    "                    {\n",
    "                        'toolResult': {\n",
    "                            'toolUseId':function_calling['toolUseId'],\n",
    "                            'content': [\n",
    "                                {\n",
    "                                    \"text\": tool_response\n",
    "                                }\n",
    "                            ],\n",
    "                            'status': tool_status\n",
    "                        }\n",
    "                    }\n",
    "                ]\n",
    "            }\n",
    "        )\n",
    "        #print(f\"\\n{datetime.now().strftime('%H:%M:%S')} - Messages so far:\\n{json.dumps(messages, indent=2)}\")\n",
    "\n",
    "        #Invoke the model one more time:\n",
    "        output = converse_with_tools(messages, system)\n",
    "        print(f\"\\n{datetime.now().strftime('%H:%M:%S')} - Final output:\\n{json.dumps(output['output'], indent=2, ensure_ascii=False)}\\n\")\n",
    "    return\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40fcfa1c-3483-4abd-a20d-a12f6e54e0f4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "prompts = [\n",
    "    \"What is the object literal for id as 'DOID_9884' and predicate as 'http://purl.obolibrary.org/obo/' \",\n",
    "    ]\n",
    "\n",
    "for prompt in prompts:\n",
    "    converse(\n",
    "        system = [{\"text\": \"You're provided with a few tools; \\\n",
    "            only use the tool if required. Don't make reference to the tools in your final answer.\"}],\n",
    "        prompt = prompt\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f3910e1-6011-4165-8c14-23a4cf816b8d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "788cb32f-0070-497d-beb1-1004af28ed97",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "availableInstances": [
   {
    "_defaultOrder": 0,
    "_isFastLaunch": true,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 4,
    "name": "ml.t3.medium",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 1,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.t3.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 2,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.t3.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 3,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.t3.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 4,
    "_isFastLaunch": true,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.m5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 5,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.m5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 6,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.m5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 7,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.m5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 8,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.m5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 9,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.m5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 10,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.m5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 11,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.m5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 12,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.m5d.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 13,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.m5d.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 14,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.m5d.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 15,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.m5d.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 16,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.m5d.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 17,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.m5d.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 18,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.m5d.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 19,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.m5d.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 20,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": true,
    "memoryGiB": 0,
    "name": "ml.geospatial.interactive",
    "supportedImageNames": [
     "sagemaker-geospatial-v1-0"
    ],
    "vcpuNum": 0
   },
   {
    "_defaultOrder": 21,
    "_isFastLaunch": true,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 4,
    "name": "ml.c5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 22,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.c5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 23,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.c5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 24,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.c5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 25,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 72,
    "name": "ml.c5.9xlarge",
    "vcpuNum": 36
   },
   {
    "_defaultOrder": 26,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 96,
    "name": "ml.c5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 27,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 144,
    "name": "ml.c5.18xlarge",
    "vcpuNum": 72
   },
   {
    "_defaultOrder": 28,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.c5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 29,
    "_isFastLaunch": true,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.g4dn.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 30,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.g4dn.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 31,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.g4dn.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 32,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.g4dn.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 33,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.g4dn.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 34,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.g4dn.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 35,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 61,
    "name": "ml.p3.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 36,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 244,
    "name": "ml.p3.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 37,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 488,
    "name": "ml.p3.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 38,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.p3dn.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 39,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.r5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 40,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.r5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 41,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.r5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 42,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.r5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 43,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.r5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 44,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.r5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 45,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 512,
    "name": "ml.r5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 46,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.r5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 47,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.g5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 48,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.g5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 49,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.g5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 50,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.g5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 51,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.g5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 52,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.g5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 53,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.g5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 54,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.g5.48xlarge",
    "vcpuNum": 192
   },
   {
    "_defaultOrder": 55,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 1152,
    "name": "ml.p4d.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 56,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 1152,
    "name": "ml.p4de.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 57,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.trn1.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 58,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 512,
    "name": "ml.trn1.32xlarge",
    "vcpuNum": 128
   },
   {
    "_defaultOrder": 59,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 512,
    "name": "ml.trn1n.32xlarge",
    "vcpuNum": 128
   }
  ],
  "instance_type": "ml.t3.medium",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
